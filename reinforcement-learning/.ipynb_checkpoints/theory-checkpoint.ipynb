{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- Theory Summary\n",
    "- Mathematical Background Summary\n",
    "- Nyang's Idea Summary\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminologies\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "\n",
    "- 랜덤 프로세스 (random process) : 시간에 따라 가변적 확률(랜덤) 변수를 가지는 경우 \n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property of Reinforcement Learning\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "#### <b> 지시(Instructs) vs 평가(Evaluates) </b>\n",
    "- 해야할 할 행동을 <b>지시(instructs)</b> 받아 훈련하는 것이 아닌, (가치적) <b>평가(evaluates)</b>를 통해 진행합니다\n",
    "- 그렇기 때문에 <b>탐험(exploration)</b>이란 과정으로 <b>시행착오</b>를 통해 좋은 행동을 <b>탐색하는 것</b>이 필수적입니다.\n",
    "- 이를 위해선 선택한 행동에 대한 결과로써 <b>평가적 피드백</b>을 받아야만 합니다. \n",
    "<br><br>\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "#### <b> 평가적 피드백(evaluative feedback) vs 지시적 피드백(instructive feedback) </b>\n",
    "- <b>평가적 피드백</b>이란 단순히옳다/그르다가 아닌 실제 취한 그 행동이 얼마나 좋은지를 나타내는 지표입니다\n",
    "- 반면 <b>지시적 피드백</b>은 실제 행해진 행동과 무관하게 '이 행동을 해야돼'라고 알려주는 것입니다\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO : 보상값 R 정의를 앞문단으로 옮길 것\n",
    "\n",
    "# Value of action (short-term)\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 각 <b>행동</b>에는 저마다 대응되는 <b>보상들의 기대값</b>이 존재하고 이를 일컬어 <b>그 행동의 가치</b>(value)라고 합니다\n",
    "- <b>시점 t</b>에서 <b>행동 a</b>에 대한 <b>가치 기대값</b>을 수학적으로 아래와 같이 표현합니다\n",
    "- $ q_{*}(a) \\doteq \\mathbb{E} \\big[R_t \\mid  A_t =a \\big] $\n",
    "<br><br>\n",
    "- 만약 행동의 가치를 모두 알고 있다면 주어진 상황에서 가장 가치가 높은 행동만을 선택하면 됩니다\n",
    "- 즉 주어진 상황에서 어떤 행동을 선택하는 기준 중에 한가지가 바로 행동의 가치입니다\n",
    "<br><br>\n",
    "- 하지만 현실에서는 처음부터 모든 각 행동들의 정확한 가치값을 알고 있는 경우는 매우 드뭅니다 \n",
    "- 따라서 어떤 방식으로든 <b>가치를 추정하는 과정이 필요</b>합니다\n",
    "<br><br>\n",
    "- <b>시점 t</b>에서 행동 a에 대한 <b>가치 근사 추정값</b>을 수학적으로 아래와 같이 표현합니다\n",
    "- $ Q_{t}(a) \\approx q_{*}(a) $\n",
    "<br><br>\n",
    "- 기대값(expected)과 추정값(estimated)의 개념을 명확히 구분하셔서 두 식의 의미적 차이를 확인하시기 바랍니다\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy or Not (exploiting vs exploring)\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 행동에 대한 가치를 추정하였다면 행동들 중에서 적어도 하나 이상의 <b>추정값이 가장 높은 행동</b>이 있을 것입니다\n",
    "- 추정값이 가장 높은 행동들을 <b>greedy actions</b>이라고 하며, 이 행동을 선택하는 것이 곧 <b>탐욕 선택법</b>이 됩니다\n",
    "<br><br>\n",
    "- 탐욕 선택법, 즉 가장 가치가 높다고 알려진 행동을 선택하는 상황을 <b>활용(exploiting)</b>이라고 합니다\n",
    "- 반면, 저평가 되었을지도 모르는 행동을(Non-Greedy) 선택해 도전과 모험을 시도하는 것을 <b>탐험/탐색(exploring)</b>이라고 합니다\n",
    "<br><br>\n",
    "- 많은 시행착오를 통해 대부분 행동들의 가치를 학습했다면, 이 연륜을 통해 보상을 최대화하는 것이 <b>활용</b>입니다\n",
    "- 당장 손해보거나 보상이 낮을 지라도 미래나 장기적으로 큰 보상으로 돌아올지 모르는 행동을 개척하는 것이 <b>탐험</b>입니다\n",
    "- 학습 초기 단계에 시행착오 경험이 부족하여 <b>행동 가치의 추정이 부정확한 경우</b> 적극적인 탐험이 필요합니다\n",
    "- 환경의 <b>변동성</b>이 크고 빨라서 기존 경험이 현재의 현상을 잘 설명하지 못하고 <b>시대에 뒤쳐질 때</b>에도 탐험이 필요합니다\n",
    "<br><br>\n",
    "- 이를 <b>탐험과 활용 딜레마(exploration exploitation dilemma)</b>라고 합니다\n",
    "- 이 딜레마는 사람이 학습하는 과정이나 능력을 사용할 때에도 비슷하게 겪는 현상입니다\n",
    "- 따라서 강화학습 훈련시 활용과 탐험이라는 이 두 가지 성질을 모두 고려해야 합니다. \n",
    "- 인간이 탐험을 잘 활용한 사례로는 딕 포스베리 배면뛰기, 금광 채굴, 임요환의 발상전환 전략 사례들이 있습니다\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Estimate Action-Value\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 어떤 행동의 <b>참 가치(true value)</b>는 해당 행동을 취했을 때 얻는 보상들의 평균(mean)입니다\n",
    "- 이 값을 추정하는 방법 중 한 가지로 실제 획득한 보상의 평균값(average)이 있습니다\n",
    "- 평균(mean)과 평균값(average)의 개념적 차이에 유념해주세요\n",
    "<br><br>\n",
    "\n",
    "- $ Q_{t}(a) \\doteq \\dfrac{시점~t까지~행동~a를~취하여~받은~보상값의~합}{시점~t까지~행동~a의~시행횟수} \n",
    "~~~~~=~~ \\dfrac{\\sum^{t-1}_{i=1}R_i \\cdot \\mathbf1_{A_{i=a}}} {\\sum^{t-1}_{i=1} \\mathbf1_{A_i=a}} $\n",
    "<br><br>\n",
    "\n",
    "- $\\mathbf1_{predicate}$는 술어 조건이 참일 때 1 아니면 0을 표현합니다 \n",
    "- 분모값이 0인 경우에 대해서는 적절한 기본값을 선정하세요\n",
    "- 분모값이 무한히 커지게 되면 <b>큰 수의 법칙</b>에 따라 $Q_{t}(a)$는 $q_{*}(a)$으로 수렴합니다\n",
    "- 이것을 샘플 평균 (Sample Average Approximation) 방식이라고 합니다\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Select Action ($\\epsilon$-greedy)\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 행동을 선택하는 가장 손쉬운 방법은 행동의 추정 가치값이 가장 높은 것 중 하나를 택하는 것입니다\n",
    "- 이것은 탐욕 선택법이며 $Q_t(A^*_t) = max_{a}\\,Q_t(a) $를 구하는 것이고 수학적으로 아래와 같이 표현합니다\n",
    "- $  A_t \\doteq \\underset{a}{\\operatorname{argmax}}Q_t(a) $\n",
    "<br><br>\n",
    "\n",
    "- 탐욕 선택은 현재까지 알려진 정보에 한정하여 즉각적 보상을 최대화하는 <b>활용(exploiting)</b> 방식입니다\n",
    "- 이 방식은 현재 저평가되었지만 더 좋은 가치의 가능성이 있는 다른 행동들을 개척할 여지를 주지 않습니다\n",
    "- 이 때문에 일정 확률로 <b>탐험</b>을 통해 행동을 선택하도록 하고 이것을 <b>e-greedy</b> 방식이라 합니다\n",
    "- 즉, 대부분의 경우 <b>활용</b>을 하지만 종종 <b>탐험</b>을 하기에 <b>near-greedy</b> 방식입니다\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='#7aa05d' />\n",
    "*** \n",
    "# 미정리 \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent & Environment\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MP (Markov Process, Markov Chain)\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDP (Markov Decision Process)\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRP (Markove Reward Process)\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Equation for MRP\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "\n",
    "- 현재 보상과 미래(onward) 보상 2 관점으로 쪼개서 보는 방식 \n",
    "<br><br>\n",
    "\n",
    "- bellman equation\n",
    "- bellman expectation equation\n",
    "- bellman optimal equation\n",
    "- 의 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components of RL MDP\n",
    "\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- State (S)\n",
    "- Reward (R)\n",
    "- Policy (P)\n",
    "- State Transition Probabitlity (T)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 보상값과 보상 함수\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 보상값 $r$ : 실제 행동을 취하고 환경으로부터 받는 보상값 $scalar~value$\n",
    "- 보상함수 $R^a_s = \\mathbb{E}\\big[R_{t+1} \\mid S_t=s, A_t=a\\big]$\n",
    "<br><br>\n",
    "- 보상함수의 암묵적 속성\n",
    "- Expectation (기대값)\n",
    "- Immediate (즉시성)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Function\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- 상태 s에 도착했을 때 해당 상태의 종료 전까지 기대할 수 있는 가치\n",
    "- $v(s) = \\mathbb{E} \\big[\\, G_t \\mid S_t=s \\,\\big]$\n",
    "\n",
    "\n",
    "<br><br>\n",
    "- 가치함수의 속성 \n",
    "- long-term value of state s (장기성)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- Policy Iteration은 '상태 가치 함수'에 대한 '벨만 기대 방정식'을 푸는 것 ${}$\n",
    "- 상태 s에서 정책에 따라 선택 가능한 모든 행동과 그에 따르는 보상 및 다음 상태를 고려 ${}$\n",
    "- 이런 과정에 따라 식에서 다음 상태항의 $S_{t+1}$가 대문자임을 유의\n",
    "- 보상 함수를 $R_s^a$로 표현하지 않은 것 역시 같은 이치 (모든 행동을 고려하겠다는 뜻) ${}$\n",
    "<br><br>\n",
    "- Iteration의 역할 (이 과정은 다음 챕터인 Value Iteration도 동일) ${}$ \n",
    "- 무한 재귀식의 값을 구하기 위해 단계별 반복을 통해 참 값으로 수렴하도록 반복 (유튭 참조) ${}$\n",
    "- 즉, 딱 다음 한 스텝의 보상과 다음 상태들의 현재 기록된 가치값을 활용 (비록 참 값이 아니지만) ${}$\n",
    "- 상태 가치 함수는 현재 내가 얼마나 잘 하고 있는지를 파악하는 도구 (알파고 승률 예측 역시)${}$\n",
    "<br><br><br>\n",
    "\n",
    "- $v_{\\pi}(s) = E_{\\pi} \\big[\\, R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} \\ldots \\mid S_t=s \\,\\big]$\n",
    "- $v_{\\pi}(s) = E_{\\pi} \\big[\\, R_{t+1} + \\gamma G(S_{t+1}) \\mid S_t=s \\,\\big]$ \n",
    "- $v_{\\pi}(s) = E_{\\pi} \\big[\\, R_{t+1} + \\gamma v_{\\pi}(S_{t+1}) \\mid S_t=s \\,\\big]$\n",
    "- $v_{\\pi}(s) = \\sum\\limits_{a \\in A}\\pi(a \\mid s)\\bigg(R_{t+1} + \\gamma \\sum\\limits_{s' \\in S}P_{ss'}^{~a} ~v_{\\pi}(S') \\bigg) $\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- Value Iteration은 '상태 가치 함수'에 대한 '벨만 최적 방정식'을 푸는 것 ${}$\n",
    "- 상태 s에서 정책 중 가장 가치가 높은 행동만 선택하여 계산 (처음부터 최적이라 가정하고 진행) ${}$\n",
    "- 별도의 정책망을 가지지 않음. (No Policy Table) ${}$\n",
    "<br><br><br>\n",
    "- $v_{*}(s) = \\max\\limits_a E\\big[R_{t+1} + \\gamma v_{*}(S_{t+1}) \\mid S_t=s,A_t=a\\big]$\n",
    "- $v_{k+1}(s) = \\max\\limits_{a \\in A}\\big(R_s^a + \\gamma v_k(s') \\big)$\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits of Dynamic Programming\n",
    "<font color='#7aa05d' />\n",
    "<br>\n",
    "- Time Complexity : $O(state_x \\times state_y \\times action_{\\pi})$ <br>\n",
    "=> 각 단계별로 모든 가능한 상태에 대해 몽땅 계산을 해버리는 한계 (큰 문제에서는 비현실적인 전략) <br>\n",
    "=> 사람처럼 실제로 듣고, 보고, 행동을 통해 경험한 상태와 정보 범위 내에서 계산하는 전략이 필요\n",
    "<br><br>\n",
    "- Curse of Dimensionality (차원의 저주) ${}$ <br>\n",
    "=> 현실의 태스크는 상태의 규모가 더 큰 경우가 대다수 (breakout : 게임 화면 이미지 크기 x RGB(3))${}$\n",
    "<br><br>\n",
    "- 시뮬레이션 환경으로부터 직접 정보를 얻어서 활용하였다는 한계. ${}$ <br>\n",
    "=> Reward, State Transition Probability가 명확히 주어짐${}$\n",
    "<br><br>\n",
    "- Environment vs Agent 경계 : 정보의 관측 여부가 아닌 제어 여부.\n",
    "- 현실처럼 환경의 정보를 알 수 없거나 제어할 수 없는 상황에서도 환경과 상호작용으로 학습이 가능한 방법 필요.\n",
    "<br><br>\n",
    "- 예측불허의 환경 예시\n",
    "- 출근길에 { 자전거, 버스, 자가용, 택시, 지하철, 항공편 } 중 빠르고 편한 것을 고민 중.\n",
    "- 지하철에 폭탄 테러가 발생하여 평생 출근이 불가능.\n",
    " <br><br>\n",
    "- <b>예측</b> : 에이전트가 행동을 통해 환경과 지속적 상호작용을 진행하여 정책의 가치를 산정하는 것 <br>\n",
    "=> 앞선 예제의 정책 평가와 유사\n",
    "<br><br>\n",
    "- <b>제어</b> : 산정된 가치를 기반으로 좋은 정책으로 발전시켜 나가는 것 <br>\n",
    "=> 앞선 예제에서 정책 발전과 유사\n",
    "<br><br>\n",
    "- <b>오프-폴리시</b> : 현재 행동하는 정책과는 별개로 학습 <br>\n",
    "=> 행동을 하기 위한 정책과 학습을 통해 개선해나가는 정책을 서로 분리\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
